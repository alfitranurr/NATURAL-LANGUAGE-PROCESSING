{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Reddit data collection...\n",
      "Processing post 1 - Current data count: 0\n",
      "Processing post 2 - Current data count: 1\n",
      "Processing post 3 - Current data count: 501\n",
      "Processing post 4 - Current data count: 643\n",
      "Processing post 5 - Current data count: 1064\n",
      "Processing post 6 - Current data count: 1508\n",
      "Processing post 7 - Current data count: 1631\n",
      "Processing post 8 - Current data count: 2108\n",
      "Processing post 9 - Current data count: 2197\n",
      "Processing post 10 - Current data count: 2249\n",
      "Processing post 11 - Current data count: 2362\n",
      "Processing post 12 - Current data count: 2857\n",
      "Processing post 13 - Current data count: 3322\n",
      "Processing post 14 - Current data count: 3517\n",
      "Processing post 15 - Current data count: 3575\n",
      "Processing post 16 - Current data count: 3608\n",
      "Processing post 17 - Current data count: 4066\n",
      "Processing post 18 - Current data count: 4142\n",
      "Processing post 19 - Current data count: 4224\n",
      "Processing post 20 - Current data count: 4723\n",
      "Processing post 21 - Current data count: 4768\n",
      "Processing post 22 - Current data count: 4853\n",
      "Processing post 23 - Current data count: 4874\n",
      "Processing post 24 - Current data count: 4973\n",
      "Processing post 25 - Current data count: 4994\n",
      "Processing post 26 - Current data count: 5017\n",
      "Processing post 27 - Current data count: 5034\n",
      "Processing post 28 - Current data count: 5058\n",
      "Processing post 29 - Current data count: 5091\n",
      "Processing post 30 - Current data count: 5513\n",
      "Processing post 31 - Current data count: 5711\n",
      "Processing post 32 - Current data count: 6185\n",
      "Processing post 33 - Current data count: 6232\n",
      "Processing post 34 - Current data count: 6243\n",
      "Processing post 35 - Current data count: 6310\n",
      "Processing post 36 - Current data count: 6352\n",
      "Processing post 37 - Current data count: 6749\n",
      "Processing post 38 - Current data count: 6776\n",
      "Processing post 39 - Current data count: 6798\n",
      "Processing post 40 - Current data count: 6862\n",
      "Processing post 41 - Current data count: 6888\n",
      "Processing post 42 - Current data count: 7187\n",
      "Processing post 43 - Current data count: 7237\n",
      "Processing post 44 - Current data count: 7240\n",
      "Processing post 45 - Current data count: 7596\n",
      "Processing post 46 - Current data count: 7795\n",
      "Processing post 47 - Current data count: 7823\n",
      "Processing post 48 - Current data count: 7840\n",
      "Processing post 49 - Current data count: 7881\n",
      "Processing post 50 - Current data count: 7899\n",
      "Processing post 51 - Current data count: 8314\n",
      "Processing post 52 - Current data count: 8340\n",
      "Processing post 53 - Current data count: 8811\n",
      "Processing post 54 - Current data count: 8980\n",
      "Processing post 55 - Current data count: 9004\n",
      "Processing post 56 - Current data count: 9105\n",
      "Processing post 57 - Current data count: 9136\n",
      "Processing post 58 - Current data count: 9172\n",
      "Processing post 59 - Current data count: 9193\n",
      "Processing post 60 - Current data count: 9218\n",
      "Processing post 61 - Current data count: 9253\n",
      "Processing post 62 - Current data count: 9722\n",
      "Processing post 63 - Current data count: 9757\n",
      "Processing post 64 - Current data count: 9767\n",
      "Processing post 65 - Current data count: 9770\n",
      "Processing post 66 - Current data count: 9846\n",
      "Processing post 67 - Current data count: 9850\n",
      "Processing post 68 - Current data count: 9926\n",
      "Processing post 69 - Current data count: 9931\n",
      "Processing post 70 - Current data count: 9970\n",
      "Saving data to files...\n",
      "Data collection completed!\n",
      "Total data collected: 10000\n",
      "Execution time: 2.88 minutes\n"
     ]
    }
   ],
   "source": [
    "import praw\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Reddit API credentials\n",
    "CLIENT_ID = os.getenv(\"CLIENT_ID\")\n",
    "CLIENT_SECRET = os.getenv(\"CLIENT_SECRET\")\n",
    "USER_AGENT = os.getenv(\"USER_AGENT\")\n",
    "\n",
    "# Inisialisasi Reddit API\n",
    "reddit = praw.Reddit(\n",
    "    client_id=CLIENT_ID,\n",
    "    client_secret=CLIENT_SECRET,\n",
    "    user_agent=USER_AGENT\n",
    ")\n",
    "\n",
    "# Subreddit yang akan di-scrape\n",
    "subreddits = ['politics']\n",
    "\n",
    "# Fungsi untuk mengumpulkan data\n",
    "def collect_reddit_data(target_count=10000):\n",
    "    data = []\n",
    "    posts_processed = 0\n",
    "    \n",
    "    for subreddit_name in subreddits:\n",
    "        subreddit = reddit.subreddit(subreddit_name)\n",
    "        \n",
    "        # Mengambil post dari kategori \"hot\"\n",
    "        for submission in subreddit.hot(limit=None):\n",
    "            if len(data) >= target_count:\n",
    "                break\n",
    "                \n",
    "            posts_processed += 1\n",
    "            print(f\"Processing post {posts_processed} - Current data count: {len(data)}\")\n",
    "            \n",
    "            # Memastikan semua komentar dimuat\n",
    "            submission.comments.replace_more(limit=0)\n",
    "            \n",
    "            # Menggunakan permalink Reddit sebagai Content_Link\n",
    "            content_link = f\"https://www.reddit.com{submission.permalink}\"\n",
    "            \n",
    "            for comment in submission.comments.list():\n",
    "                if len(data) >= target_count:\n",
    "                    break\n",
    "                    \n",
    "                try:\n",
    "                    comment_data = {\n",
    "                        'Subreddit': subreddit_name,\n",
    "                        'Post_Title': submission.title,\n",
    "                        'Username': comment.author.name if comment.author else '[deleted]',\n",
    "                        'Comment': comment.body,\n",
    "                        'Score': comment.score,  # Upvotes - Downvotes\n",
    "                        'Content_Link': content_link,  # Hanya URL Reddit\n",
    "                        'Timestamp': datetime.fromtimestamp(comment.created_utc).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                    }\n",
    "                    data.append(comment_data)\n",
    "                    \n",
    "                except AttributeError as e:\n",
    "                    print(f\"Error processing comment: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Delay untuk menghindari rate limit\n",
    "            time.sleep(1)\n",
    "            \n",
    "        if len(data) >= target_count:\n",
    "            break\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Fungsi untuk menyimpan ke TXT\n",
    "def save_to_txt(data, filename=\"reddit_data.txt\"):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        for item in data:\n",
    "            f.write(f\"Subreddit: {item['Subreddit']}\\n\")\n",
    "            f.write(f\"Post Title: {item['Post_Title']}\\n\")\n",
    "            f.write(f\"Username: {item['Username']}\\n\")\n",
    "            f.write(f\"Comment: {item['Comment']}\\n\")\n",
    "            f.write(f\"Score: {item['Score']}\\n\")\n",
    "            f.write(f\"Content Link: {item['Content_Link']}\\n\")\n",
    "            f.write(f\"Timestamp: {item['Timestamp']}\\n\")\n",
    "            f.write(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "# Fungsi untuk menyimpan ke CSV\n",
    "def save_to_csv(data, filename=\"reddit_data.csv\"):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, index=False, encoding='utf-8')\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    print(\"Starting Reddit data collection...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Mengumpulkan data\n",
    "    collected_data = collect_reddit_data()\n",
    "    \n",
    "    # Menyimpan data\n",
    "    print(\"Saving data to files...\")\n",
    "    save_to_txt(collected_data)\n",
    "    save_to_csv(collected_data)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    execution_time = (end_time - start_time) / 60  # dalam menit\n",
    "    \n",
    "    print(f\"Data collection completed!\")\n",
    "    print(f\"Total data collected: {len(collected_data)}\")\n",
    "    print(f\"Execution time: {execution_time:.2f} minutes\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memulai deteksi data unik di kolom Subreddit...\n",
      "Data Unik di Kolom Subreddit:\n",
      "------------------------------\n",
      "politics\n",
      "------------------------------\n",
      "Total jumlah subreddit unik: 1\n",
      "Proses selesai!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Fungsi untuk mendeteksi data unik di kolom Subreddit\n",
    "def detect_unique_subreddits(csv_file=\"reddit_data.csv\"):\n",
    "    try:\n",
    "        # Membaca file CSV\n",
    "        df = pd.read_csv(csv_file)\n",
    "        \n",
    "        # Mengekstrak nilai unik dari kolom 'Subreddit'\n",
    "        unique_subreddits = df['Subreddit'].unique()\n",
    "        \n",
    "        # Menampilkan hasil\n",
    "        print(\"Data Unik di Kolom Subreddit:\")\n",
    "        print(\"-\" * 30)\n",
    "        for subreddit in unique_subreddits:\n",
    "            print(subreddit)\n",
    "        print(\"-\" * 30)\n",
    "        print(f\"Total jumlah subreddit unik: {len(unique_subreddits)}\")\n",
    "        \n",
    "        return unique_subreddits\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {csv_file} tidak ditemukan. Pastikan file sudah ada.\")\n",
    "        return None\n",
    "    except KeyError:\n",
    "        print(\"Kolom 'Subreddit' tidak ditemukan di file CSV.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Terjadi kesalahan: {e}\")\n",
    "        return None\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    print(\"Memulai deteksi data unik di kolom Subreddit...\")\n",
    "    unique_subreddits = detect_unique_subreddits()\n",
    "    \n",
    "    if unique_subreddits is not None:\n",
    "        print(\"Proses selesai!\")\n",
    "    else:\n",
    "        print(\"Proses gagal.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
